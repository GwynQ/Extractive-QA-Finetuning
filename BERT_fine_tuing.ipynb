{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install -q transformers datasets accelerate evaluate","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForQuestionAnswering,\n    TrainingArguments,\n    Trainer,\n    DefaultDataCollator\n)\nfrom datasets import load_dataset\nimport numpy as np\nfrom tqdm.auto import tqdm\nimport json\nimport random\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim import AdamW\n\n# Fix random seed for reproducibility\ndef same_seeds(seed):\n\ttorch.manual_seed(seed)\n\tif torch.cuda.is_available():\n\t\t\ttorch.cuda.manual_seed(seed)\n\t\t\ttorch.cuda.manual_seed_all(seed)\n\tnp.random.seed(seed)\n\trandom.seed(seed)\n\ttorch.backends.cudnn.benchmark = False\n\ttorch.backends.cudnn.deterministic = True\nsame_seeds(2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T02:59:12.439634Z","iopub.execute_input":"2025-12-26T02:59:12.440261Z","iopub.status.idle":"2025-12-26T02:59:15.782929Z","shell.execute_reply.started":"2025-12-26T02:59:12.440233Z","shell.execute_reply":"2025-12-26T02:59:15.782356Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Check GPU availability\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T02:59:15.784284Z","iopub.execute_input":"2025-12-26T02:59:15.784831Z","iopub.status.idle":"2025-12-26T02:59:15.789730Z","shell.execute_reply.started":"2025-12-26T02:59:15.784805Z","shell.execute_reply":"2025-12-26T02:59:15.788906Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nGPU: Tesla T4\nMemory: 15.83 GB\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Configuration\nMODEL_NAME = \"bert-base-uncased\"  # Can also try distilbert-base-uncased for faster training\nMAX_LENGTH = 384\nSTRIDE = 128\nBATCH_SIZE = 8  # Adjust based on GPU memory\nLEARNING_RATE = 1e-5\nEPOCHS = 1\nOUTPUT_DIR = \"./qa_model\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T02:59:15.791019Z","iopub.execute_input":"2025-12-26T02:59:15.791752Z","iopub.status.idle":"2025-12-26T02:59:15.825366Z","shell.execute_reply.started":"2025-12-26T02:59:15.791714Z","shell.execute_reply":"2025-12-26T02:59:15.824576Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"print(\"\\n=== Loading Tokenizer and Model ===\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForQuestionAnswering.from_pretrained(MODEL_NAME)\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-25T10:32:10.267078Z","iopub.status.idle":"2025-12-25T10:32:10.267449Z","shell.execute_reply.started":"2025-12-25T10:32:10.267286Z","shell.execute_reply":"2025-12-25T10:32:10.267311Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import BertTokenizerFast, BertForQuestionAnswering\n\ntokenizer = BertTokenizerFast.from_pretrained(\"deepset/bert-base-cased-squad2\")\nmodel = BertForQuestionAnswering.from_pretrained(\"deepset/bert-base-cased-squad2\")\nmodel.to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T02:59:19.124031Z","iopub.execute_input":"2025-12-26T02:59:19.124375Z","iopub.status.idle":"2025-12-26T02:59:19.907440Z","shell.execute_reply.started":"2025-12-26T02:59:19.124350Z","shell.execute_reply":"2025-12-26T02:59:19.906829Z"}},"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at deepset/bert-base-cased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"BertForQuestionAnswering(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"print(\"\\n=== Loading Dataset ===\")\n# Load SQuAD dataset (using subset for faster training on free GPU)\ndataset = load_dataset(\"squad\", split=\"train\")  # Use full dataset: split=\"train\"\neval_dataset = load_dataset(\"squad\", split=\"validation[500:2500]\")  # Use split=\"validation\" for full\n\nprint(f\"Training samples: {len(dataset)}\")\nprint(f\"Validation samples: {len(eval_dataset)}\")\nprint(f\"\\nExample: {dataset[0]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T02:59:21.872311Z","iopub.execute_input":"2025-12-26T02:59:21.872981Z","iopub.status.idle":"2025-12-26T02:59:25.302557Z","shell.execute_reply.started":"2025-12-26T02:59:21.872950Z","shell.execute_reply":"2025-12-26T02:59:25.301860Z"}},"outputs":[{"name":"stdout","text":"\n=== Loading Dataset ===\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11ff633b9f2541bba6b470b24ca32533"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"plain_text/train-00000-of-00001.parquet:   0%|          | 0.00/14.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95eff868217049ab8fefadf62072e380"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"plain_text/validation-00000-of-00001.par(…):   0%|          | 0.00/1.82M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ecb37f5c84440dbbcc0527075d20aa9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0be9add0a3af4086b659723af4a9f63c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75ab379a2308491caf3a9e41a330e72f"}},"metadata":{}},{"name":"stdout","text":"Training samples: 87599\nValidation samples: 2000\n\nExample: {'id': '5733be284776f41900661182', 'title': 'University_of_Notre_Dame', 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"class SquadDataset(torch.utils.data.Dataset):\n    \"\"\"\n    Dataset wrapper for SQuAD format\n    SQuAD format: {'id', 'title', 'context', 'question', 'answers': {'text': [...], 'answer_start': [...]}}\n    \"\"\"\n    \n    def __init__(self, squad_data, tokenizer, max_length=384, is_training=True):\n        self.data = squad_data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.is_training = is_training\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        item = self.data[idx]\n        \n        question = item['question']\n        context = item['context']\n        \n        # Tokenize with offsets to map tokens back to character positions\n        encoding = self.tokenizer(\n            question,\n            context,\n            truncation=True,\n            max_length=self.max_length,\n            stride=128,  # For handling long contexts\n            padding=\"max_length\",\n            return_overflowing_tokens=False,  # Keep only first chunk for simplicity\n            return_offsets_mapping=True,\n            return_token_type_ids=True\n        )\n        \n        input_ids = torch.tensor(encoding[\"input_ids\"])\n        attention_mask = torch.tensor(encoding[\"attention_mask\"])\n        token_type_ids = torch.tensor(encoding[\"token_type_ids\"])\n        offset_mapping = encoding[\"offset_mapping\"]\n        \n        result = {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"token_type_ids\": token_type_ids,\n        }\n        \n        # ALWAYS add start and end positions (even for evaluation)\n        # This ensures compute_metrics works properly\n        answers = item['answers']\n        \n        # SQuAD can have multiple answers, take the first one\n        if len(answers['text']) > 0:\n            answer_text = answers['text'][0]\n            answer_start_char = answers['answer_start'][0]\n            answer_end_char = answer_start_char + len(answer_text)\n            \n            # Find token positions\n            start_position, end_position = self.char_to_token_position(\n                offset_mapping,\n                answer_start_char,\n                answer_end_char,\n                encoding.get(\"sequence_ids\", None)\n            )\n        else:\n            # Unanswerable question (SQuAD v2)\n            start_position = 0\n            end_position = 0\n        \n        result[\"start_positions\"] = torch.tensor(start_position, dtype=torch.long)\n        result[\"end_positions\"] = torch.tensor(end_position, dtype=torch.long)\n        \n        # Keep original data for text-level evaluation\n        result[\"context\"] = context\n        result[\"question\"] = question\n        result[\"ground_truth\"] = item['answers']['text'][0] if len(item['answers']['text']) > 0 else \"\"\n        result[\"example_id\"] = item['id']\n        \n        return result\n    \n    def char_to_token_position(self, offset_mapping, start_char, end_char, sequence_ids=None):\n        \"\"\"\n        Convert character positions to token positions\n        \"\"\"\n        start_position = 0\n        end_position = 0\n        \n        for idx, (start, end) in enumerate(offset_mapping):\n            # Skip special tokens and question tokens\n            if start == 0 and end == 0:\n                continue\n            \n            # If we have sequence_ids, only look in context (sequence_id == 1)\n            if sequence_ids is not None and sequence_ids[idx] == 0:\n                continue  # This is in the question part\n            \n            # Find start position\n            if start <= start_char < end:\n                start_position = idx\n            \n            # Find end position\n            if start < end_char <= end:\n                end_position = idx\n        \n        # Ensure valid span\n        if end_position < start_position:\n            end_position = start_position\n        \n        return start_position, end_position","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T02:59:25.303849Z","iopub.execute_input":"2025-12-26T02:59:25.304094Z","iopub.status.idle":"2025-12-26T02:59:25.315304Z","shell.execute_reply.started":"2025-12-26T02:59:25.304061Z","shell.execute_reply":"2025-12-26T02:59:25.314573Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"print(\"\\n=== Creating Dataset Objects ===\")\n\ntrain_dataset = SquadDataset(\n    squad_data=dataset,\n    tokenizer=tokenizer,\n    max_length=384,  # Standard for BERT-based models\n    is_training=True\n)\n\nvalidation_dataset = SquadDataset(\n    squad_data=eval_dataset,\n    tokenizer=tokenizer,\n    max_length=384,\n    is_training=False\n)\n\nprint(f\"✓ Train dataset: {len(train_dataset)} samples\")\nprint(f\"✓ Validation dataset: {len(validation_dataset)} samples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T02:59:25.905073Z","iopub.execute_input":"2025-12-26T02:59:25.905775Z","iopub.status.idle":"2025-12-26T02:59:25.910380Z","shell.execute_reply.started":"2025-12-26T02:59:25.905743Z","shell.execute_reply":"2025-12-26T02:59:25.909706Z"}},"outputs":[{"name":"stdout","text":"\n=== Creating Dataset Objects ===\n✓ Train dataset: 87599 samples\n✓ Validation dataset: 2000 samples\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from sklearn.model_selection import KFold\n\nshuffled_dataset = train_dataset.shuffle(seed=42)\n\nk = 5\nkf = KFold(n_splits=k, shuffle=True, random_state=42)\n\n# Convert train_data to a list if not already\ntrain_data_list = list(dataset)\n\nfor fold, (train_idx, val_idx) in enumerate(kf.split(train_data_list)):\n    train_fold = [train_data_list[i] for i in train_idx]\n    val_fold = [train_data_list[i] for i in val_idx]\n    \n    print(f\"Fold {fold+1}: Train={len(train_fold)}, Val={len(val_fold)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Verify a sample\nprint(\"\\n=== Verifying Dataset Format ===\")\nsample = train_dataset[0]\nprint(f\"Sample keys: {list(sample.keys())}\")\nprint(f\"✓ input_ids shape: {sample['input_ids'].shape}\")\nprint(f\"✓ attention_mask shape: {sample['attention_mask'].shape}\")\nprint(f\"✓ token_type_ids shape: {sample['token_type_ids'].shape}\")\nif 'start_positions' in sample:\n    print(f\"✓ start_positions: {sample['start_positions'].item()}\")\n    print(f\"✓ end_positions: {sample['end_positions'].item()}\")\nprint(f\"✓ ground_truth: {sample['ground_truth'][:50]}...\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Verify a sample\nprint(\"\\n=== Verifying Dataset Format ===\")\nsample = validation_dataset[0]\nprint(f\"Sample keys: {list(sample.keys())}\")\nprint(f\"✓ input_ids shape: {sample['input_ids'].shape}\")\nprint(f\"✓ attention_mask shape: {sample['attention_mask'].shape}\")\nprint(f\"✓ token_type_ids shape: {sample['token_type_ids'].shape}\")\nif 'start_positions' in sample:\n    print(f\"✓ start_positions: {sample['start_positions'].item()}\")\n    print(f\"✓ end_positions: {sample['end_positions'].item()}\")\nprint(f\"✓ ground_truth: {sample['ground_truth'][:50]}...\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class QADataCollator:\n    \"\"\"\n    Custom data collator for QA that handles both training and evaluation\n    \"\"\"\n    \n    def __call__(self, features):\n        # Separate tensor fields from non-tensor fields\n        batch = {\n            \"input_ids\": torch.stack([f[\"input_ids\"] for f in features]),\n            \"attention_mask\": torch.stack([f[\"attention_mask\"] for f in features]),\n        }\n        \n        # Add token_type_ids if present\n        if \"token_type_ids\" in features[0]:\n            batch[\"token_type_ids\"] = torch.stack([f[\"token_type_ids\"] for f in features])\n        \n        # Add position labels if present (training mode)\n        if \"start_positions\" in features[0]:\n            batch[\"start_positions\"] = torch.stack([f[\"start_positions\"] for f in features])\n            batch[\"end_positions\"] = torch.stack([f[\"end_positions\"] for f in features])\n        \n        # Keep non-tensor fields separate (for evaluation)\n        if \"context\" in features[0]:\n            batch[\"contexts\"] = [f[\"context\"] for f in features]\n            batch[\"questions\"] = [f[\"question\"] for f in features]\n            batch[\"ground_truths\"] = [f[\"ground_truth\"] for f in features]\n        \n        return batch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T02:59:31.837166Z","iopub.execute_input":"2025-12-26T02:59:31.837459Z","iopub.status.idle":"2025-12-26T02:59:31.843467Z","shell.execute_reply.started":"2025-12-26T02:59:31.837432Z","shell.execute_reply":"2025-12-26T02:59:31.842810Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def postprocess_qa_predictions(\n    examples,\n    features,\n    raw_predictions,\n    n_best_size=20,\n    max_answer_length=30,\n):\n    \"\"\"\n    Convert start/end logits into final text answers.\n    \"\"\"\n    all_start_logits, all_end_logits = raw_predictions\n\n    example_id_to_index = {ex[\"id\"]: i for i, ex in enumerate(examples)}\n    features_per_example = collections.defaultdict(list)\n\n    for i, feature in enumerate(features):\n        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n\n    predictions = collections.OrderedDict()\n\n    for example_index, example in enumerate(examples):\n        feature_indices = features_per_example[example_index]\n        context = example[\"context\"]\n\n        best_score = -1e9\n        best_answer = \"\"\n\n        for feature_index in feature_indices:\n            start_logits = all_start_logits[feature_index]\n            end_logits = all_end_logits[feature_index]\n            offsets = features[feature_index][\"offset_mapping\"]\n\n            start_indexes = np.argsort(start_logits)[-n_best_size:]\n            end_indexes = np.argsort(end_logits)[-n_best_size:]\n\n            for start_index in start_indexes:\n                for end_index in end_indexes:\n                    if start_index >= len(offsets) or end_index >= len(offsets):\n                        continue\n                    if offsets[start_index] is None or offsets[end_index] is None:\n                        continue\n                    if end_index < start_index:\n                        continue\n                    if end_index - start_index + 1 > max_answer_length:\n                        continue\n\n                    score = start_logits[start_index] + end_logits[end_index]\n                    if score > best_score:\n                        start_char = offsets[start_index][0]\n                        end_char = offsets[end_index][1]\n                        best_answer = context[start_char:end_char]\n                        best_score = score\n\n        predictions[example[\"id\"]] = best_answer\n\n    return predictions\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T03:00:33.371683Z","iopub.execute_input":"2025-12-26T03:00:33.372436Z","iopub.status.idle":"2025-12-26T03:00:33.380507Z","shell.execute_reply.started":"2025-12-26T03:00:33.372406Z","shell.execute_reply":"2025-12-26T03:00:33.379639Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"!pip install -q evaluate","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T03:00:35.739492Z","iopub.execute_input":"2025-12-26T03:00:35.739794Z","iopub.status.idle":"2025-12-26T03:00:41.230604Z","shell.execute_reply.started":"2025-12-26T03:00:35.739767Z","shell.execute_reply":"2025-12-26T03:00:41.229836Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import evaluate\nmetric = evaluate.load(\"squad\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T03:00:41.232084Z","iopub.execute_input":"2025-12-26T03:00:41.232612Z","iopub.status.idle":"2025-12-26T03:00:43.018400Z","shell.execute_reply.started":"2025-12-26T03:00:41.232580Z","shell.execute_reply":"2025-12-26T03:00:43.017857Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d82930382c94265b4c51ea7a3f2a183"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a67ff1ed80ae4669b0c270f3a7ab2871"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"def compute_metrics(eval_pred):\n    start_logits, end_logits = eval_pred.predictions\n    predictions = []\n    references = []\n\n    for i in range(len(validation_dataset)):\n        example = validation_dataset[i]\n        input_ids = example[\"input_ids\"]\n        context = example[\"context\"]\n        offsets = tokenizer(\n            example[\"question\"],\n            context,\n            return_offsets_mapping=True,\n            truncation=True,\n            max_length=384\n        )[\"offset_mapping\"]\n\n        start = int(np.argmax(start_logits[i]))\n        end = int(np.argmax(end_logits[i]))\n\n        if start <= end and start < len(offsets) and end < len(offsets):\n            start_char = offsets[start][0]\n            end_char = offsets[end][1]\n            pred_text = context[start_char:end_char]\n        else:\n            pred_text = \"\"\n\n        predictions.append({\n            \"id\": example[\"example_id\"],\n            \"prediction_text\": pred_text\n        })\n\n        references.append({\n            \"id\": example[\"example_id\"],\n            \"answers\": {\n                \"text\": [example[\"ground_truth\"]],\n                \"answer_start\": [0]\n            }\n        })\n\n    return metric.compute(predictions=predictions, references=references)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T03:00:48.211979Z","iopub.execute_input":"2025-12-26T03:00:48.212637Z","iopub.status.idle":"2025-12-26T03:00:48.218931Z","shell.execute_reply.started":"2025-12-26T03:00:48.212606Z","shell.execute_reply":"2025-12-26T03:00:48.218258Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from transformers import get_linear_schedule_with_warmup\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n\nnum_training_steps = len(train_dataset) / BATCH_SIZE / 8 * EPOCHS\nnum_warmup_steps = int(0.2 * num_training_steps)\n\nscheduler = get_linear_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=num_warmup_steps,\n    num_training_steps=num_training_steps\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T03:00:57.466242Z","iopub.execute_input":"2025-12-26T03:00:57.466553Z","iopub.status.idle":"2025-12-26T03:00:57.472036Z","shell.execute_reply.started":"2025-12-26T03:00:57.466524Z","shell.execute_reply":"2025-12-26T03:00:57.471498Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=OUTPUT_DIR,\n    #eval_strategy=\"epoch\",\n    #save_strategy=\"epoch\",\n    learning_rate=LEARNING_RATE,\n    eval_strategy=\"steps\",\n    save_strategy=\"steps\",\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    num_train_epochs=EPOCHS,\n    weight_decay=0.05,\n    fp16=True,\n    logging_steps=100,\n    gradient_accumulation_steps=4,\n    warmup_ratio=0.2,\n    #load_best_model_at_end=True,\n    #metric_for_best_model=\"eval_span_exact_match\",  # Use text-level EM\n    #greater_is_better=True,  # Higher EM is better\n    push_to_hub=False,\n    report_to=\"none\",\n    save_total_limit=2,\n    label_names=[]\n)\n\n# Create data collator\ndata_collator = QADataCollator()\n#data_collator = DefaultDataCollator()\n\n# Create trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=validation_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    optimizers=(optimizer, scheduler),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T03:00:59.574669Z","iopub.execute_input":"2025-12-26T03:00:59.575521Z","iopub.status.idle":"2025-12-26T03:00:59.630951Z","shell.execute_reply.started":"2025-12-26T03:00:59.575488Z","shell.execute_reply":"2025-12-26T03:00:59.630237Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_55/1078618425.py:31: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"trainer.can_return_loss = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T03:01:02.763198Z","iopub.execute_input":"2025-12-26T03:01:02.763841Z","iopub.status.idle":"2025-12-26T03:01:02.767239Z","shell.execute_reply.started":"2025-12-26T03:01:02.763815Z","shell.execute_reply":"2025-12-26T03:01:02.766595Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"print(\"\\n=== Starting Training ===\")\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T03:01:03.307493Z","iopub.execute_input":"2025-12-26T03:01:03.308120Z","iopub.status.idle":"2025-12-26T04:14:39.329668Z","shell.execute_reply.started":"2025-12-26T03:01:03.308089Z","shell.execute_reply":"2025-12-26T04:14:39.328770Z"}},"outputs":[{"name":"stdout","text":"\n=== Starting Training ===\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1369' max='1369' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1369/1369 1:13:30, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Exact Match</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>0.671300</td>\n      <td>1.210883</td>\n      <td>64.450000</td>\n      <td>78.383069</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.506500</td>\n      <td>1.210193</td>\n      <td>65.200000</td>\n      <td>79.302727</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.500300</td>\n      <td>1.185845</td>\n      <td>65.400000</td>\n      <td>79.291602</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.481300</td>\n      <td>1.201964</td>\n      <td>66.000000</td>\n      <td>79.690336</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.505000</td>\n      <td>1.189681</td>\n      <td>65.850000</td>\n      <td>79.458609</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.492000</td>\n      <td>1.188847</td>\n      <td>65.350000</td>\n      <td>79.362426</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.495400</td>\n      <td>1.209417</td>\n      <td>65.400000</td>\n      <td>79.554896</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.499600</td>\n      <td>1.190094</td>\n      <td>65.650000</td>\n      <td>79.555728</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.500900</td>\n      <td>1.178088</td>\n      <td>66.000000</td>\n      <td>79.865284</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.468100</td>\n      <td>1.195574</td>\n      <td>66.050000</td>\n      <td>79.754233</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.482500</td>\n      <td>1.196711</td>\n      <td>66.050000</td>\n      <td>79.900114</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.475900</td>\n      <td>1.188971</td>\n      <td>66.050000</td>\n      <td>79.828584</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.447300</td>\n      <td>1.196450</td>\n      <td>66.150000</td>\n      <td>80.059399</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.12/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1369, training_loss=0.5007549533885791, metrics={'train_runtime': 4415.5491, 'train_samples_per_second': 19.839, 'train_steps_per_second': 0.31, 'total_flos': 1.7167000944987648e+16, 'train_loss': 0.5007549533885791, 'epoch': 1.0})"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"print(\"\\n=== Saving Model ===\")\ntrainer.save_model(OUTPUT_DIR)\ntokenizer.save_pretrained(OUTPUT_DIR)\nprint(f\"Model saved to {OUTPUT_DIR}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T04:17:01.706138Z","iopub.execute_input":"2025-12-26T04:17:01.706461Z","iopub.status.idle":"2025-12-26T04:17:02.489151Z","shell.execute_reply.started":"2025-12-26T04:17:01.706434Z","shell.execute_reply":"2025-12-26T04:17:02.488391Z"}},"outputs":[{"name":"stdout","text":"\n=== Saving Model ===\nModel saved to ./qa_model\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"print(\"\\n=== Testing the Model ===\")\n# Load the fine-tuned model\nqa_model = AutoModelForQuestionAnswering.from_pretrained(OUTPUT_DIR)\nqa_model.to(device)\nqa_model.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T04:17:02.490401Z","iopub.execute_input":"2025-12-26T04:17:02.490615Z","iopub.status.idle":"2025-12-26T04:17:02.702775Z","shell.execute_reply.started":"2025-12-26T04:17:02.490594Z","shell.execute_reply":"2025-12-26T04:17:02.702187Z"}},"outputs":[{"name":"stdout","text":"\n=== Testing the Model ===\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"BertForQuestionAnswering(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"import re\n\ndef normalize_text(s):\n    \"\"\"Normalize text for comparison\"\"\"\n    s = s.lower()\n    s = re.sub(r'[^\\w\\s]', '', s)\n    return ' '.join(s.split())\n\ndef compute_exact_match(prediction, ground_truth):\n    \"\"\"Calculate exact match\"\"\"\n    return max([float(normalize_text(prediction).replace(\" \", \"\") == normalize_text(ground_truth[i]).replace(\" \", \"\").strip(\".\")) for i in range(len(ground_truth))])\n\ndef compute_f1(prediction, ground_truth):\n    f1_arr = [0] * len(ground_truth)\n    for i in range(len(ground_truth)):\n        \"\"\"Calculate F1 score\"\"\"\n        pred_tokens = normalize_text(prediction).split()\n        truth_tokens = normalize_text(ground_truth[i]).split()\n    \n        if not pred_tokens or not truth_tokens:\n            f1_arr[i] = float(pred_tokens == truth_tokens)\n            continue\n    \n        common = set(pred_tokens) & set(truth_tokens)\n        if not common:\n            f1_arr[i] = 0.0\n            continue\n    \n        precision = len(common) / len(pred_tokens)\n        recall = len(common) / len(truth_tokens)\n        f1_arr[i] = 2 * (precision * recall) / (precision + recall)\n    return max(f1_arr)\n\ndef compute_qa_metrics(eval_pred):\n    \"\"\"\n    Compute metrics for extractive QA\n    Note: This computes span-level accuracy, not text-level EM/F1\n    \"\"\"\n    predictions, labels = eval_pred\n    start_logits, end_logits = predictions\n    start_positions, end_positions = labels\n    \n    # Get predicted positions\n    pred_start = np.argmax(start_logits, axis=-1)\n    pred_end = np.argmax(end_logits, axis=-1)\n    \n    # Calculate exact match at span level\n    start_match = (pred_start == start_positions).astype(float)\n    end_match = (pred_end == end_positions).astype(float)\n    exact_match = (start_match * end_match).mean()\n    \n    # Calculate partial match (either start or end correct)\n    partial_match = np.maximum(start_match, end_match).mean()\n    \n    return {\n        \"span_exact_match\": float(exact_match),\n        \"span_partial_match\": float(partial_match),\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T04:17:06.130602Z","iopub.execute_input":"2025-12-26T04:17:06.130893Z","iopub.status.idle":"2025-12-26T04:17:06.139583Z","shell.execute_reply.started":"2025-12-26T04:17:06.130866Z","shell.execute_reply":"2025-12-26T04:17:06.138858Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"test_data = load_dataset(\"squad\", split=\"validation[:500]\")\ntest_dataset = SquadDataset(\n        squad_data=test_data,\n        tokenizer=tokenizer,\n        max_length=384,\n        is_training=False\n    )\nprint(f\"✓ Loaded {len(test_dataset)} test examples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T04:17:08.890578Z","iopub.execute_input":"2025-12-26T04:17:08.890864Z","iopub.status.idle":"2025-12-26T04:17:09.979755Z","shell.execute_reply.started":"2025-12-26T04:17:08.890837Z","shell.execute_reply":"2025-12-26T04:17:09.979129Z"}},"outputs":[{"name":"stdout","text":"✓ Loaded 500 test examples\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"def answer_question(question, context):\n    inputs = tokenizer(\n        question,\n        context,\n        max_length=MAX_LENGTH,\n        truncation=True,\n        return_offsets_mapping=True,\n        return_tensors=\"pt\"\n    )\n\n    offset_mapping = inputs.pop(\"offset_mapping\")[0]\n    inputs = inputs.to(device)\n\n    with torch.no_grad():\n        outputs = qa_model(**inputs)\n        #outputs = model(**inputs)\n\n    start_idx = torch.argmax(outputs.start_logits).item()\n    end_idx = torch.argmax(outputs.end_logits).item()\n\n    start_char = offset_mapping[start_idx][0]\n    end_char = offset_mapping[end_idx][1]\n\n    answer = context[start_char:end_char]\n    return answer\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T04:17:25.669001Z","iopub.execute_input":"2025-12-26T04:17:25.669339Z","iopub.status.idle":"2025-12-26T04:17:25.674849Z","shell.execute_reply.started":"2025-12-26T04:17:25.669303Z","shell.execute_reply":"2025-12-26T04:17:25.674076Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def answer_question_test(question, text):\n    device = model.device\n    \n    inputs = tokenizer(question, text, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        outputs = qa_model(**inputs)\n    \n    answer_start_index = outputs.start_logits.argmax()\n    answer_end_index = outputs.end_logits.argmax()\n    predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n    \n    answer = tokenizer.decode(predict_answer_tokens, skip_special_tokens=True)\n    return answer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T04:17:27.860888Z","iopub.execute_input":"2025-12-26T04:17:27.861231Z","iopub.status.idle":"2025-12-26T04:17:27.866304Z","shell.execute_reply.started":"2025-12-26T04:17:27.861200Z","shell.execute_reply":"2025-12-26T04:17:27.865524Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"import pandas as pd\n\nnum_test_samples = min(500, len(test_data))  # Adjust as needed\n\nexact_matches = []\nf1_scores = []\nrows = []\n\nfor i in range(num_test_samples):\n    example = test_data[i]\n    context = example['context']\n    question = example['question']\n    ground_truth = example['answers']['text']\n\n    # Generate prediction\n    prediction = answer_question_test(question, context).strip()\n\n    # Compute metrics\n    em = compute_exact_match(prediction, ground_truth)\n    f1 = compute_f1(prediction, ground_truth)\n\n    exact_matches.append(em)\n    f1_scores.append(f1)\n\n    rows.append({\n        \"prediction\": prediction,\n        \"ground_truth\": ground_truth,\n        \"exact_match\": em,\n        \"f1\": f1\n    })\n\n    # Show some examples\n    if i < 5:\n        print(f\"\\nExample {i+1}:\")\n        print(f\"Question: {question}\")\n        print(f\"Ground Truth: {ground_truth}\")\n        print(f\"Prediction: {prediction}\")\n        print(f\"Exact Match: {em} | F1: {f1:.3f}\")\n\n# Print overall metrics\nprint(\"\\n\" + \"=\"*60)\nprint(\"OVERALL TEST RESULTS\")\nprint(\"=\"*60)\nprint(f\"Exact Match Accuracy: {sum(exact_matches)/len(exact_matches)*100:.2f}%\")\nprint(f\"Average F1 Score: {sum(f1_scores)/len(f1_scores)*100:.2f}%\")\nprint(f\"Samples Evaluated: {num_test_samples}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T04:17:30.543885Z","iopub.execute_input":"2025-12-26T04:17:30.544433Z","iopub.status.idle":"2025-12-26T04:17:37.145141Z","shell.execute_reply.started":"2025-12-26T04:17:30.544403Z","shell.execute_reply":"2025-12-26T04:17:37.144507Z"}},"outputs":[{"name":"stdout","text":"\nExample 1:\nQuestion: Which NFL team represented the AFC at Super Bowl 50?\nGround Truth: ['Denver Broncos', 'Denver Broncos', 'Denver Broncos']\nPrediction: Denver Broncos\nExact Match: 1.0 | F1: 1.000\n\nExample 2:\nQuestion: Which NFL team represented the NFC at Super Bowl 50?\nGround Truth: ['Carolina Panthers', 'Carolina Panthers', 'Carolina Panthers']\nPrediction: Carolina Panthers\nExact Match: 1.0 | F1: 1.000\n\nExample 3:\nQuestion: Where did Super Bowl 50 take place?\nGround Truth: ['Santa Clara, California', \"Levi's Stadium\", \"Levi's Stadium in the San Francisco Bay Area at Santa Clara, California.\"]\nPrediction: Levi's Stadium\nExact Match: 1.0 | F1: 1.000\n\nExample 4:\nQuestion: Which NFL team won Super Bowl 50?\nGround Truth: ['Denver Broncos', 'Denver Broncos', 'Denver Broncos']\nPrediction: Denver Broncos\nExact Match: 1.0 | F1: 1.000\n\nExample 5:\nQuestion: What color was used to emphasize the 50th anniversary of the Super Bowl?\nGround Truth: ['gold', 'gold', 'gold']\nPrediction: golden\nExact Match: 0.0 | F1: 0.000\n\n============================================================\nOVERALL TEST RESULTS\n============================================================\nExact Match Accuracy: 84.00%\nAverage F1 Score: 83.08%\nSamples Evaluated: 500\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"df = pd.DataFrame(rows)\n\noutput_path = \"squad_predictions.xlsx\"\ndf.to_excel(output_path, index=False)\n\nprint(f\"\\nSaved prediction results to {output_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-26T04:17:56.530252Z","iopub.execute_input":"2025-12-26T04:17:56.530569Z","iopub.status.idle":"2025-12-26T04:17:57.278677Z","shell.execute_reply.started":"2025-12-26T04:17:56.530540Z","shell.execute_reply":"2025-12-26T04:17:57.278007Z"}},"outputs":[{"name":"stdout","text":"\nSaved prediction results to squad_predictions.xlsx\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"def compute_exact_match(prediction, ground_truth):\n    \"\"\"Check if prediction exactly matches ground truth\"\"\"\n    return prediction == ground_truth\n\ndef compute_f1(prediction, ground_truth):\n    \"\"\"Compute F1 score between prediction and ground truth\"\"\"\n    pred_tokens = prediction.split()\n    truth_tokens = ground_truth.split()\n\n    common = set(pred_tokens) & set(truth_tokens)\n    num_same = len(common)\n\n    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n        return int(pred_tokens == truth_tokens)\n\n    if num_same == 0:\n        return 0\n\n    precision = num_same / len(pred_tokens)\n    recall = num_same / len(truth_tokens)\n    f1 = (2 * precision * recall) / (precision + recall)\n\n    return f1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def decode_prediction(example, start_logit, end_logit):\n    encoding = tokenizer(\n        example[\"question\"],\n        example[\"context\"],\n        return_offsets_mapping=True,\n        truncation=True,\n        max_length=384\n    )\n\n    offsets = encoding[\"offset_mapping\"]\n\n    start_idx = int(start_logit.argmax())\n    end_idx = int(end_logit.argmax())\n\n    if start_idx >= len(offsets) or end_idx >= len(offsets) or start_idx > end_idx:\n        return \"\"\n\n    start_char = offsets[start_idx][0]\n    end_char = offsets[end_idx][1]\n\n    return example[\"context\"][start_char:end_char]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T17:29:23.217628Z","iopub.execute_input":"2025-12-24T17:29:23.217950Z","iopub.status.idle":"2025-12-24T17:29:23.223085Z","shell.execute_reply.started":"2025-12-24T17:29:23.217903Z","shell.execute_reply":"2025-12-24T17:29:23.222458Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\nloader = DataLoader(\n    test_dataset,\n    batch_size=8,\n    shuffle=False\n)\n\npredictions = []\nreferences = []\n\nwith torch.no_grad():\n    for batch in loader:\n        input_ids = batch[\"input_ids\"].cuda()\n        attention_mask = batch[\"attention_mask\"].cuda()\n        token_type_ids = batch[\"token_type_ids\"].cuda()\n\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids\n        )\n\n        for i in range(len(input_ids)):\n            example_id = batch[\"example_id\"][i]\n            context = batch[\"context\"][i]\n            question = batch[\"question\"][i]\n            gt = batch[\"ground_truth\"][i]\n\n            example = {\n                \"context\": context,\n                \"question\": question\n            }\n\n            pred_text = decode_prediction(\n                example,\n                outputs.start_logits[i],\n                outputs.end_logits[i]\n            )\n\n            predictions.append({\n                \"id\": example_id,\n                \"prediction_text\": pred_text\n            })\n\n            references.append({\n                \"id\": example_id,\n                \"answers\": {\n                    \"text\": [gt],\n                    \"answer_start\": [0]\n                }\n            })\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"results = squad_metric.compute(\n    predictions=predictions,\n    references=references\n)\n\nprint(\"Exact Match:\", results[\"exact_match\"])\nprint(\"F1:\", results[\"f1\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-24T17:29:30.985547Z","iopub.execute_input":"2025-12-24T17:29:30.986125Z","iopub.status.idle":"2025-12-24T17:29:30.993135Z","shell.execute_reply.started":"2025-12-24T17:29:30.986096Z","shell.execute_reply":"2025-12-24T17:29:30.992235Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(5):\n    print(\"Q:\", references[i][\"id\"])\n    print(\"GT:\", references[i][\"answers\"][\"text\"][0])\n    print(\"Pred:\", predictions[i][\"prediction_text\"])\n    print(\"-\" * 50)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
